<!DOCTYPE html>
<html lang="en">


<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="UMMAFormer">

    <title>UMMAFormer - Project Page</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/charts.css/dist/charts.min.css">
    <link id="theme-style" rel="stylesheet" href="./assets/UMMAFormer/css/UMMAFormer_main.css">
    <link id="theme-style" rel="stylesheet" href="./assets/UMMAFormer/css/bulma-carousel.min.css">
    <link id="theme-style" rel="stylesheet" href="./assets/UMMAFormer/css/bulma-slider.min.css">

    <!-- <script type="module" src="./assets/js/background_box.js"></script> -->
    <script type="module" src="./assets/UMMAFormer/js/background_star.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./assets/UMMAFormer/js/bulma-carousel.min.js"></script>
    <script src="./assets/UMMAFormer/js/bulma-slider.min.js"></script>
    <script src="./assets/UMMAFormer/js/index.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-D41DLSC2BJ');
    </script>
</head>

<body>

    <div class="wrapper">

        <section class="section intro-section">
            <div class="intro-container" style="text-align: center;">
                <div class="header">
                    <h3 class="papername">UMMAFormer: A Universal Multimodal-adaptive Transformer Framework For Temporal Forgery Localization
                    </h3>
                </div>
                <ul class="list-unstyled name-list" style="font-size:large;">
                    <li><a href="ymhzyj.github.io/" target="_blank">Rui Zhang</a></li>
                    <li><a href="https://ccs.scu.edu.cn/info/1052/2601.htm" target="_blank">Hongxia Wang<sup>2</sup></a></li>
                    <li>Mingshan Du</li>
                    <li>Yang Zhou</li>
                    <li>Qiang Zeng</li>
                </ul>
                <ul class="list-unstyled name-list" style="font-size:large;">
                    <li>School of Cyber Science and Engineering, Sichuan University</li>
                </ul>
                <div class="title"> <a href="https://www.acmmm2023.org/#" target="_blank">ACM MM 2023</a></div>
                <br>
                <div style="font-size:large;">
                    Traditional methods mainly focus on image forgery classification and video forgery classification, aiming to determine whether an entire video 
                    is real or manipulated. In contrast, <b>UMMAFormer</b> emphasizes temporal forgery localization, specifically targeting and identifying manipulated 
                    segments within a video.
                </div>

            </div>
            <br>
            <div class="intro-container" style="text-align: center;">
                <!-- <img src='./assets/UMMAFormer/images/problem_definition.gif' style="width: 80%" /> -->
                <!-- <img src='./assets/UMMAFormer/images/problem_definition.png' onmouseover="this.src='./assets/UMMAFormer/images/problem_definition.gif';" onmouseout="this.src='./assets/UMMAFormer/images/problem_definition.png';"  style="width: 80%" /> -->
                <img src='./assets/UMMAFormer/images/intro_v9.png' style="width: 80%" />
            </div>
        </section>

        <section class='section'>
            <div class="section-title">
                Abstract
            </div>
            <div class="details" style="text-align: justify" ;>
                The emergence of artificial intelligence-generated content (AIGC) has raised 
                concerns about the authenticity of multimedia content in various fields. 
                However, existing research for forgery content detection has mainly focused on binary 
                classification tasks of complete videos, which has limited applicability in industrial settings. 
                To address this gap, we propose UMMAFormer, a novel universal transformer framework for temporal forgery 
                localization (TFL) that predicts forgery segments with multimodal adaptation. Our approach introduces a 
                Temporal Feature Abnormal Attention (TFAA) module based on temporal feature reconstruction to enhance the 
                detection of temporal differences. We also design a Parallel Cross-attention Feature Pyramid Network (PCA-FPN) 
                to optimize the Feature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the proposed method, 
                we contribute a novel Temporal Video Inpainting Localization (TVIL) dataset specifically tailored for video 
                inpainting scenes. Our experiments show that our approach achieves state-of-the-art performance on benchmark datasets, 
                including Lav-DF, TVIL, and Psynd, outperforming previous methods significantly.
                
            </div>
        </section>

        <section class='section links-section'>
            <div class='section-title'>
                Links
            </div>
            <div class='details links-table'>
                <table>
                    <tr>
                        <!-- <td>
                            <div class='links-container'>
                                <a href='https://arxiv.org/abs/2304.02556' target="_blank"><img class='links-cover'
                                        src='./assets/UMMAFormer/images/PDF_icon.svg' alt='PDF Cover' style="width: 35%"></a>
                            </div>
                        </td> -->
                        <!-- <td>
                            <div class='links-container'>
                                <a href='https://youtu.be/EortO0cqnGE' target="_blank"><img class='links-cover'
                                        src='./assets/UMMAFormer/images/youtube_icon.png' alt='youtube icon'></a>
                            </div>
                        </td> -->
                        <td>
                            <div class='links-container'>
                                <a href='https://github.com/ymhzyj/UMMAFormer' target="_blank"><img class='links-cover'
                                        src='./assets/UMMAFormer/images/github.png' alt='github icon'></a>
                            </div>
                        </td>
                        <td>
                            <div class='links-container'>
                                <a href='https://pan.baidu.com/s/1h3sHu56z3slJnPCH47QRsg?pwd=8tj1' target="_blank"><img class='links-cover'
                                        src='./assets/UMMAFormer/images/baiduyundisk.jpg' alt='baiduyundisk icon'></a>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <!-- <td><a href='https://arxiv.org/abs/2304.02556' target="_blank">Paper</a></td>
                        <td><a href='https://youtu.be/EortO0cqnGE' target="_blank">Video</a></td> -->
                        <td><a href='https://github.com/ymhzyj/UMMAFormer' target="_blank">Code</a></td>
                        <td><a href='https://pan.baidu.com/s/1h3sHu56z3slJnPCH47QRsg?pwd=8tj1' target="_blank">Dataset</a>
                        </td>
                    </tr>
                </table>
            </div>
        </section>


        <!-- Paper video. -->
        <!-- <section class="section intro-section">
            <div class="section-title">
                Video
            </div>
            <div class="details" style="text-align: center;">
                <div class="publication-video">
                    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/EortO0cqnGE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
                    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden;">
                        <iframe style="position:absolute; top:0; left:0; width:100%; height:100%;" src="https://www.youtube.com/embed/EortO0cqnGE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </div>
                </div>

            <!-- <div style="display: flex;justify-content: center;align-items: center;max-width: 1000px;margin: 0 auto;">
                <iframe style="max-width: 1000px;height: auto;" src="https://www.youtube.com/embed/EortO0cqnGE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
                 -->
            </div>
        </section> -->


        <!-- <section class="section">
            <div class="section-title">
                Video
            </div>
            <div class="details">
                <iframe class="video" src="https://www.youtube.com/embed/EortO0cqnGE" title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen=""></iframe>
            </div>
        </section> -->
        <!-- / Paper video.   -->



    <!-- <br>
        <section class="section">
            <div class="section-title">
                DGM<sup>4</sup> Dataset
            </div>
            <div class="details" style="text-align: center;">
                <img class='links-cover' src='./assets/UMMAFormer/images/statistics_v12.jpg' , width="80%">
            </div>

        <br>
            <p>We present <b>DGM<sup>4</sup></b>, a large-scale dataset for studying machine-generated multi-modal media manipulation. 
                The dataset specifically focus on <t>human-centric news</t>, in consideration of its great public influence.
                It consists a total of <b>230k</b> news samples, including 77,426 pristine image-text pairs and 152,574 manipulated pairs. 
                The manipulated pairs contain:
            </p>
            <ul>
                <!-- <li>66,722 <span style="color:#5a9bd4">Face Swap Manipulations (FS)</span></li>
                <li>56,411 <span style="color:#9ec2e7">Face Attribute Manipulations (FA)</span></li>
                <li>43,546 <span style="color:#febf04">Text Swap Manipulations (TS)</span></li>
                <li>18,588 <span style="color:#feda65">Text Attribute Manipulations (TA)</span></li> -->
                <li>66,722 Face Swap Manipulations <b>(FS)</b></li>
                <li>56,411 Face Attribute Manipulations <b>(FA)</b></li>
                <li>43,546 Text Swap Manipulations <b>(TS)</b></li>
                <li>18,588 Text Attribute Manipulations <b>(TA)</b></li>
                <!-- <ul>
                    <li><b>35,166</b> number of face images</li>
                    <li><b>28</b> types of manipulation sequences</li>
                </ul> -->
                <!-- <li><span style="color:#0a939d">Sequential facial attributes manipulation</span></li> -->
                <!-- <ul>
                    <li><b>49,920</b> number of face images</li>
                    <li><b>26</b> types of manipulation sequences</li>
                </ul> -->
            </ul>
            <p>
                Where 1/3 of the manipulated images and 1/2 of the manipulated text are combined together to form 32,693 mixed-manipulation pairs.
            </p>

            <p>
                Some sample images and their annotations are shown below. 
                For more information about the data structure, annotation details and other properties about the dataset, 
                you can refer to our 
                <a href="https://github.com/rshaojimmy/UMMAFormer" target="_blank">github page</a></li>.
            </p>
        <br>
            <!-- <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/supp_dataset_v2.jpg' style="width: 90%" />
            </div> -->
            <section class="section intro-section">
                <div id="carouselExampleIndicators" class="video-container">
                    <div id="results-carousel" class="carousel is-dark results-carousel video-container-inner">

                        <div class="video item">
                            <img src='./assets/UMMAFormer/images/dataset_1.PNG'
                            style="width: 100%">
                            <!-- <p class="video-caption">
                                <span style="color:#0a939d">Bangs-Smiling</span>
                            </p> -->
                        </div>

                        <div class="video item">
                            <img src='./assets/UMMAFormer/images/dataset_2.PNG'
                            style="width: 100%">
                            <!-- <p class="video-caption">
                                <span style="color:#AE2011">eyebrow-nose</span>
                            </p> -->
                        </div>

                        <div class="video item">
                            <img src='./assets/UMMAFormer/images/dataset_3.PNG'
                            style="width: 100%">
                            <!-- <p class="video-caption">
                                <span style="color:#0a939d">Eyeglasses</span>
                            </p> -->
                        </div>         

                    </div>
                </div>
            </section>
        </section>

        <section class='section'>
            <div class="section-title">
                Method
            </div>
            <strong><span style="font-size:larger">Proposed HAMMER</span></strong>
            <p>
                Figure below shows the architecture of proposed <b>H</b>ier<b>A</b>rchical <b>M</b>ulti-modal <b>M</b>anipulation r<b>E</b>asoning t<b>R</b>ansformer (<b>HAMMER</b>).
                It 1) aligns image and text embeddings through manipulation-aware contrastive learning 
                between Image Encoder <i>E<sub>v</sub></i>, Text Encoder <i>E<sub>t</sub></i> in <b>shallow manipulation reasoning</b> and 
                2) further aggregates multi-modal embeddings via modality-aware cross-attention 
                of Multi-Modal Aggregator <i>F</i> in <b>deep manipulation reasoning</b>. 
                Based on the interacted multi-modal embeddings in different levels, 
                various manipulation detection and grounding heads (Multi-Label Classifier 
                <i>C<sub>m</sub></i>, Binary Classifier <i>C<sub>b</sub></i>, BBox Detector <i>D<sub>v</sub></i>, and Token Detector <i>D<sub>t</sub></i>) 
                are integrated to perform their tasks hierarchically.
                <!-- Figure below shows the architecture of proposed Seq-DeepFake Transformer (<b>SeqFakeFormer</b>). 
                We first feed the face image into a CNN to learn features of spatial manipulation regions, 
                and extract their <b>spatial relation</b> via self-attention modules in the encoder. 
                Then <b>sequential relation</b> based on features of spatial relation is modeled through 
                cross-attention modules deployed in the decoder with an auto-regressive mechanism, detecting the sequential facial manipulation. 
                A spatial enhanced cross-attention module is integrated into the decoder, contributing to a more effective cross-attention. -->
            </p>
        <br>
            <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/framework_v6.jpg' style="width: 90%" />
            </div>


            
            <br>
            <br>
            <strong><span style="font-size:larger">Benchmark results</span></strong>
            <p>
                Based on the DGM<sup>4</sup> dataset, we provide the first benchmark for evaluating model performance on our proposed task.
                To validate the effectiveness of our HAMMER model, we adapt SOTA multi-modal learning methods to our DGM<sup>4</sup> setting for full-modal comparision, 
                and further adapt deepfake detection and sequence tagging methods for single-modal comparison.
                As shown in the tables, HAMMER outperforms all multi-modal and single-modal methods in all sub-tasks, under all evaluation metrics.
                <!-- We tabulate the first benchmark for detecting sequential facial manipulation in Table 1~3. 
                SeqFakeFormer outperforms all SOTA deepfake detection methods in both manipulation types, 
                under two evaluation metrics. -->
            </p>

            <br>

            <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/table_2.jpg' style="width: 75%" />
            </div>
            <br>
            <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/table_3.jpg' style="width: 40%" />
                <img src='./assets/UMMAFormer/images/table_4.jpg' style="width: 40%" />
            </div>
            

            <br>
            <br>
            <strong><span style="font-size:larger">Visualization results</span></strong>
            <br>
            <p>
                <b>Visualization of detection and grounding results.</b> Ground truth annotations are in <span style="color:#ff0000"><b>red</b></span>, 
                and prediction results are in <span style="color:#00b0f0"><b>blue</b></span>. The visualization results show that 
                our method can accurately <b>ground</b> the manipulated bboxes & text tokens, and successfully <b>detect</b> the manipulation types.
            </p>
            <br>
            <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/visualization.png'
                            style="width: 80%">
            </div>
            
            <br>
            <p>
                <b>Visualization of attention map.</b> 
                We plot Grad-CAM visualizations with respect to specifc text tokens (in <span style="color:#00b200"><b>green</b></span>) for all the four manipulated types.
                For FS and FA, we visualize the attention map regarding some key words related to image manipulation. 
                For TS and TA, we visualize the attention map regarding the manipulated text tokens. 
                The attention maps show our model can use text to facilitate locating manipulated image regions, 
                and capture subtle semantic inconsistencies between the two modalities to tackle DGM<sup>4</sup>.
            </p>
            <br>
            <div class="details" style="text-align: center" ;>
                <img src='./assets/UMMAFormer/images/attn.png'
                            style="width: 80%">
            </div>



            

        </section>

        <section class="section">
            <div class="section-title">
                Bibtex
            </div>
            <div class="details">
                <pre>
@inproceedings{shao2023dgm4,
    title={Detecting and Grounding Multi-Modal Media Manipulation},
    author={Shao, Rui and Wu, Tianxing and Liu, Ziwei},
    booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023}
}
                </pre>
            </div>
        </section>


        <section class='section'>
            <div class="section-title">
                Acknowledgement
            </div>
            <!-- <p>This study is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry
                Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and
                in-kind contribution from the industry partner(s).</p> -->
            <p>We referred to the project page of <a href="https://hongfz16.github.io/projects/AvatarCLIP.html">AvatarCLIP</a> when creating this
                project page.</p>
        </section>

    </div> -->


</body>

</html>